{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código procedente de Kaggle.\n",
    "***Autor:***\n",
    "* **LIU SHI CAI · 5Y AGO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install keras\n",
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e5d71ebffaaa868cf3173b24a32fb478cf8b0d16"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "7161040562b0e2263dea00d7955878d8095cc2d3"
   },
   "outputs": [],
   "source": [
    "# leemos los archivos CSV utilizando la biblioteca pandas.\n",
    "# Los datos se almacenan en dos DataFrames: train_df y test_df.\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "4b34d23b93170bbb6731fcc24df61c94254bbb82"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3832516617.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    train = train_df[,1:]/255\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Separamos los datos de entrenamiento en dos matrices NumPy diferentes: labels y train.\n",
    "labels = train_df[:,0]\n",
    "\n",
    "# Se dividen por 255 para realizar una normalización de los píxeles, asumiendo que los valores originales están en el rango de 0 a 255.\n",
    "train = train_df[,1:]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "ae485338a42fccce0650a635e3df713b4539f1a4"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 31.4 GiB for an array with shape (32970000, 256) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# La función to_categorical de Keras se utiliza para convertir las etiquetas numéricas en codificación one-hot.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dummy_y \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mto_categorical(labels)\n\u001b[0;32m      4\u001b[0m \u001b[39m# La función train_test_split de la biblioteca scikit-learn se utiliza para dividir los datos en conjuntos de entrenamiento y prueba.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Se utiliza un tamaño de prueba del 10% (test_size=0.1).\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Se establece una semilla aleatoria (random_state=166) para que la división sea reproducible.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# La opción stratify=labels garantiza que las proporciones de las clases se mantengan en los conjuntos de entrenamiento y prueba.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(train, dummy_y, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m166\u001b[39m,stratify\u001b[39m=\u001b[39mlabels)\n",
      "File \u001b[1;32mc:\\Users\\Me\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m     num_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(y) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     72\u001b[0m n \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((n, num_classes), dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m     74\u001b[0m categorical[np\u001b[39m.\u001b[39marange(n), y] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[39m=\u001b[39m input_shape \u001b[39m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 31.4 GiB for an array with shape (32970000, 256) and data type float32"
     ]
    }
   ],
   "source": [
    "# La función to_categorical de Keras se utiliza para convertir las etiquetas numéricas en codificación one-hot.\n",
    "dummy_y = keras.utils.to_categorical(labels)\n",
    "\n",
    "# La función train_test_split de la biblioteca scikit-learn se utiliza para dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "# Se utiliza un tamaño de prueba del 10% (test_size=0.1).\n",
    "# Se establece una semilla aleatoria (random_state=166) para que la división sea reproducible.\n",
    "# La opción stratify=labels garantiza que las proporciones de las clases se mantengan en los conjuntos de entrenamiento y prueba.\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, dummy_y, test_size=0.1, random_state=166,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f04d72367c7fe501dfbf7ff64d5ed64c04c81e7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Las matrices x_train y x_test se remodelan utilizando el método reshape para adaptarse a un formato específico antes de ser utilizadas \n",
    "en un modelo de clasificación de imágenes.\n",
    "\n",
    "La forma original de los datos en MNIST es de imágenes en escala de grises de 28x28 píxeles. Sin embargo, para entrenar un modelo de \n",
    "aprendizaje profundo, generalmente se requiere que los datos estén en un formato específico, que es\n",
    "[número_de_ejemplos, ancho, alto, canales].\n",
    "\n",
    "En este caso, se está agregando una dimensión adicional a los datos utilizando reshape. El primer argumento x_train.shape[0] indica el número\n",
    "de ejemplos de entrenamiento, y luego se especifica la forma deseada de las imágenes, que es (28, 28, 1). La dimensión adicional de 1 se refiere\n",
    "al canal de color y representa la escala de grises.\n",
    "'''\n",
    "# Remodelar los datos de entrenamiento y prueba.\n",
    "x_train = x_train.reshape(x_train.shape[0],28,28, 1)\n",
    "\n",
    "# El ancho y alto son 28x28 píxeles.\n",
    "# Y los canales son 1, que representa la escala de grises.\n",
    "x_test = x_test.reshape(x_test.shape[0],28,28, 1)\n",
    "\n",
    "'''\n",
    "Dado que el módulo de lime que estamos utilizando solo funciona con imágenes en 3D, es decir, una imagen que tiene 3 canales generalmente\n",
    "RGB, replicamos el plano en escala de grises aquí. Este segmento de código convierte una imagen en escala de grises a RGB simplemente\n",
    "replicando el plano disponible.\n",
    "'''\n",
    "# Creamos la función que adapta los datos.\n",
    "import numpy as np\n",
    "def to_rgb(x):\n",
    "    x_rgb = np.zeros((x.shape[0], 28, 28, 3))\n",
    "    for i in range(3):\n",
    "        x_rgb[..., i] = x[..., 0]\n",
    "    return x_rgb\n",
    "\n",
    "# Convertimos los datos al formato que posteriormente necesitaremos.\n",
    "x_train = to_rgb(x_train)\n",
    "x_test = to_rgb(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "079cfbd31af1406537c3698a3eaf1c0ddb77e11e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definimos y entrenamos un modelo de red neuronal convolucional utilizando Keras.\n",
    "\n",
    "\n",
    "# Creamos una instancia del modelo secuencial de Keras, que permite agregar capas de manera secuencial.\n",
    "model = Sequential()\n",
    "\n",
    "# Se define una lista de callbacks, en este caso se utiliza el ModelCheckpoint para guardar el mejor modelo durante el entrenamiento. \n",
    "# El modelo se guarda en el archivo \"convolutional_minist.h5\" basado en la métrica de precisión en la validación (val_acc).\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('cnn_minist.h5', monitor='val_accuracy', verbose=1, save_best_only=True,\n",
    "                            mode='auto')]\n",
    "\n",
    "'''Se agregan capas convolucionales utilizando la función add() del modelo secuencial. Se utilizan cuatro capas \n",
    "  convolucionales con diferentes tamaños de kernel y funciones de activación 'relu'.\n",
    "'''\n",
    "\n",
    "# La primera capa tiene un tamaño de kernel de (3,3) y recibe la forma de entrada (28,28,1) para imágenes en escala de grises.\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu',padding='same',\n",
    "                 input_shape=(28,28,3))) # MODIFICAMOS LA ENTRADA RESPECTO DEL ORIGINAL (28, 28, **3**).\n",
    "\n",
    "# Las siguientes capas convolucionales tienen el mismo tamaño de kernel (3,3) y 'padding' 'same' para mantener el tamaño de la imagen.\n",
    "model.add(Conv2D(64, (3, 3),padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "\n",
    "# La cuarta capa convolucional tiene un tamaño de kernel de (28,28) y se utiliza una activación 'relu'.\n",
    "model.add(Conv2D(128, (28, 28),activation='relu'))\n",
    "\n",
    "# Se agrega una capa de aplanamiento (flatten) para convertir los mapas de características en un vector unidimensional.\n",
    "model.add(Flatten())\n",
    "\n",
    "# Se agrega una capa densa con 256 neuronas y activación 'relu'.\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Se agrega una capa de dropout con una tasa de dropout del 50% para regularizar el modelo y evitar el sobreajuste.\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Se agrega una capa densa final con 10 neuronas (correspondientes a las 10 clases de salida). \n",
    "# Una función de activación 'softmax' para la clasificación multiclase.\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# Función utilizada en Keras para mostrar un resumen del modelo de la red neuronal. \n",
    "# Nos da una descripción de la arquitectura de la red, la forma de entrada y salida de las capa y número de parámetros entrenables del modelo.\n",
    "model.summary()\n",
    "\n",
    "# Se compila el modelo especificando la función de pérdida ('categorical_crossentropy').\n",
    "# Se define además, el optimizador ('sgd' con una tasa de aprendizaje de 0.01 y momentum de 0.9) y las métricas a utilizar ('accuracy').\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9) # \"lr\" en desuso cambiarlo por \"learning_rate\".\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Se ajusta el modelo utilizando los datos de entrenamiento.\n",
    "# Se especifica el tamaño del lote, el número de épocas, la verbosidad y los datos de validación (x_test y y_test). \n",
    "# También se pasan los callbacks definidos anteriormente.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que se utiliza para cargar los pesos pre-entrenados de un modelo guardado en un archivo HDF5.\n",
    "model.load_weights('cnn_minist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2a33bcb15ac4b5b62cb540cd9cd57e6391a0e2d"
   },
   "outputs": [],
   "source": [
    "# Le damos el formato adecuado a los datos de prueba (test) antes de realizar predicciones con el modelo.\n",
    "# Al realizar esta transformación en los datos de prueba, se asegura que tengan el mismo formato que los datos de entrenamiento.\n",
    "test = test_data.reshape(test_data.shape[0],28,28, 1)/255\n",
    "\n",
    "test = to_rgb(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e1ad38f6523a23cbd84ec324c794ff590c75ed7a"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las predicciones del modelo sobre los datos de prueba.\n",
    "predict = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0724c7fd80af2aae489f0a5469d8a2eeecfd71f0"
   },
   "outputs": [],
   "source": [
    "# Obtenemos las etiquetas predichas a partir de las probabilidades de salida del modelo.\n",
    "results = np.argmax(predict,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ff4b8968bee29140b94c864598230a4d9bc1a45"
   },
   "outputs": [],
   "source": [
    "# Creamos un DataFrame, \"cnn_submission\" que contiene dos columnas: \"ImageId\" y \"Label\". \n",
    "# La columna \"ImageId\" contiene los valores del rango del 1 al 28000 (indicando el número de imagen correspondiente), \n",
    "# La columna \"Label\" contiene las etiquetas predichas para cada imagen.\n",
    "submission = pd.DataFrame({\"ImageId\":range(1,28001),\"Label\":results})\n",
    "submission.to_csv(\"cnn_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c14d1c3cb01c550b2d75c9afc3ab6eab50727a1"
   },
   "outputs": [],
   "source": [
    "# Publicamos en pantalla el DataFrame \"submission\".\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que el modelo se ha guardado adecuadamente.\n",
    "cnn_minist = load_model('cnn_minist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_minist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
